# @package training
# Those arguments defines the training hyper-parameters
epochs: 310
num_workers: 4
batch_size: 32
cuda: 0
shuffle: True
optim:
  base_lr: 0.001
  grad_clip: 100
  optimizer:
#    class: SGD
#    params:
#      momentum: 0.98
#      lr: ${training.optim.base_lr} # The path is cut from training
#      weight_decay: 1e-3
    class: AdamW
    params:
      lr: ${training.optim.base_lr} # The path is cut from training
      weight_decay: 1e-2
  lr_scheduler: ${lr_scheduler}
#  bn_scheduler:
#    bn_policy: "step_decay"
#    params:
#      bn_momentum: 0.98
#      bn_decay: 0.9
#      decay_step: 1000
#      bn_clip: 1e-2
weight_name: "latest" # Used during resume, select with model to load from [miou, macc, acc..., latest]
enable_cudnn: True
checkpoint_dir: ""

# Those arguments within experiment defines which model, dataset and task to be created for benchmarking
# parameters for Weights and Biases
wandb:
  project: nfi
  log: True
  name: ${model_name}
  public: True # It will be display the model within wandb log, else not.
  config:
    model_name: ${model_name}
    features: ${data.features}
    batch_size: ${training.batch_size}
    first_subsampling: ${data.first_subsampling}
    base_lr: ${training.optim.base_lr}
    first_return_only: ${data.first_return_only}
    activation: ${models.${model_name}.activation}
    first_stride: ${models.${model_name}.first_stride}


# parameters for TensorBoard Visualization
tensorboard:
  log: False

enable_mixed: True
