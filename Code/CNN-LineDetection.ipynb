{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acaca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import laspy\n",
    "#import open3d as o3d\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df25c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPathRelations(full_path_to_data):        \n",
    "    ground_removed_image_paths = []\n",
    "    laz_point_cloud_paths = []\n",
    "        \n",
    "    # Find full path to all images\n",
    "    for path in glob.glob(full_path_to_data+'data/ImagesGroundRemovedSmall/*'):\n",
    "        ground_removed_image_paths.append(path)\n",
    "    \n",
    "    # Find full path to all laz files\n",
    "    for path in glob.glob(full_path_to_data+'data/LazFilesWithHeightRemoved/*'):\n",
    "        laz_point_cloud_paths.append(path)\n",
    "            \n",
    "    ground_removed_image_paths.sort()\n",
    "    laz_point_cloud_paths.sort()\n",
    "    assert(len(ground_removed_image_paths)==len(laz_point_cloud_paths))\n",
    "    return ground_removed_image_paths, laz_point_cloud_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373dfc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344bc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMinNormalize(arr):\n",
    "    return (arr - np.min(arr))/(np.max(arr)-np.min(arr))\n",
    "\n",
    "def CastAllXValuesToImage(arr, x_pixels):\n",
    "    return (MaxMinNormalize(arr))*x_pixels\n",
    "\n",
    "def CastAllYValuesToImage(arr, y_pixels):\n",
    "    return (1-MaxMinNormalize(arr))*y_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44b7573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1000 1000\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1000 1000\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1001\n",
      "4095\n",
      "1001 1000\n",
      "4095\n",
      "996 1000\n",
      "10 10\n",
      "3 3\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "all_path_relations = GetPathRelations(\"/home/frederik/data/TestData/\")\n",
    "path_tuples = list(zip(*all_path_relations))\n",
    "\n",
    "transform_img_gray = transforms.Compose(\n",
    "    [transforms.Resize((1001,1001)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Open images\n",
    "trainingImages = []\n",
    "labelImages = []\n",
    "    \n",
    "for path in path_tuples:\n",
    "    image_path, laz_path = path\n",
    "    \n",
    "    # Image to training set\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    image = np.where(image >= 0, image, 0)\n",
    "    image = image/np.max(image)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    x_pixels, y_pixels = image.shape\n",
    "    \n",
    "    image = Image.fromarray(image)\n",
    "    trainingImages.append(transform_img_gray(image))\n",
    "    \n",
    "    # Generate labels \n",
    "    las = laspy.read(laz_path, laz_backend=laspy.compression.LazBackend.LazrsParallel)\n",
    "    \n",
    "    print(x_pixels, y_pixels)\n",
    "    \n",
    "    y_values = np.rint(CastAllXValuesToImage(las.X, y_pixels)).astype(np.int32)\n",
    "    x_values = np.rint(CastAllYValuesToImage(las.Y, x_pixels)).astype(np.int32)\n",
    "    \n",
    "    powerline_mask = (las.classification == 14)\n",
    "    x_powerline_values = x_values[powerline_mask]\n",
    "    x_powerline_values = np.where(x_powerline_values < x_pixels, x_powerline_values, x_pixels-1)\n",
    "    x_powerline_values = np.where(x_powerline_values >= 0, x_powerline_values, 0)\n",
    "    \n",
    "    y_powerline_values = y_values[powerline_mask]\n",
    "    y_powerline_values = np.where(y_powerline_values < y_pixels, y_powerline_values, y_pixels-1)\n",
    "    y_powerline_values = np.where(y_powerline_values >= 0, y_powerline_values, 0)\n",
    "    \n",
    "    labels = np.zeros((x_pixels, y_pixels)).astype(np.uint8)\n",
    "    for i in range(len(x_powerline_values)):\n",
    "        labels[x_powerline_values[i], y_powerline_values[i]] = 255\n",
    "        \n",
    "    \n",
    "    linesP = cv2.HoughLinesP(\n",
    "            labels, # Input edge image\n",
    "            10, # Distance resolution in pixels\n",
    "            np.pi/180, # Angle resolution in radians\n",
    "            threshold=0, # Min number of votes for valid line\n",
    "            minLineLength=0, # Min allowed length of line\n",
    "            maxLineGap=10\n",
    "    )\n",
    "    lines_image = np.zeros_like(labels)\n",
    "    # Draw the lines\n",
    "    if linesP is not None:\n",
    "        for i in range(0, len(linesP)):\n",
    "            l = linesP[i][0]\n",
    "            cv2.line(lines_image, (l[0], l[1]), (l[2], l[3]), (255,0,0), 3)\n",
    "    \n",
    "    \n",
    "    # Create kernel\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    lines_image = cv2.dilate(lines_image, kernel, iterations=1)\n",
    "    \n",
    "#     fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(15,10))\n",
    "#     ax0.set_title('Labels')\n",
    "#     ax0.imshow(labels, cmap='gray')\n",
    "#     ax1.set_title('Hough Line')\n",
    "#     ax1.imshow(lines_image, cmap='gray')\n",
    "#     ax0.axis('off')\n",
    "#     ax1.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "    lines_image = Image.fromarray(lines_image)\n",
    "    labelImages.append(transform_img_gray(lines_image))\n",
    "    \n",
    "\n",
    "    \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(trainingImages, labelImages, test_size=0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)\n",
    "\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_test), len(Y_test))\n",
    "print(len(X_val), len(Y_val))\n",
    "\n",
    "\n",
    "# Setting up sets for trainlodader and validation loader\n",
    "training_set = []\n",
    "for i in range(len(X_train)):\n",
    "    training_set.append([X_train[i], Y_train[i]])\n",
    "    \n",
    "validation_set = []\n",
    "for i in range(len(Y_val)):\n",
    "    validation_set.append([X_val[i], Y_val[i]])\n",
    "    \n",
    "test_set = []\n",
    "for i in range(len(Y_test)):\n",
    "    test_set.append([X_test[i], Y_test[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_set, batch_size=1, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(validation_set, batch_size=1, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0444885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n",
      "torch.Size([1, 1, 1001, 1001])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    # get the input\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d71def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetRGB, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding = 1),\n",
    "            nn.ReLU()).cuda()\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding = 1),\n",
    "            nn.ReLU()).cuda()\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)).cuda()\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.down1 = nn.ConvTranspose2d(in_channels=256, out_channels=64,\n",
    "                                        stride = 2, kernel_size=3, padding = 1, output_padding=1).cuda()\n",
    "        \n",
    "        self.down2 = nn.ConvTranspose2d(in_channels=64, out_channels=16,\n",
    "                                        kernel_size=3, stride=1, padding=1).cuda()\n",
    "        \n",
    "        self.down3 = nn.ConvTranspose2d(in_channels=16, out_channels=1,\n",
    "                                        kernel_size=3, stride=1, padding=1).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.down1(out)\n",
    "        out = self.down2(out)\n",
    "        out = self.down3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90c2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNetTraining(trainloader, valloader, Conv, lossFunction,learning_rate, epochs):\n",
    "    model = Conv\n",
    "    num_epochs = epochs\n",
    "    criterion = lossFunction.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # loss arrays for figures\n",
    "    TrainingLossArray = []\n",
    "    ValidationLossArray = []\n",
    "    \n",
    "    early_stopping = 20\n",
    "    notImproved = 0\n",
    "    bestLoss = None\n",
    "    bestModel = None\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the input\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize            \n",
    "            outputs = model(inputs.cuda())\n",
    "            loss = criterion(outputs.cuda(), labels.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Appending the mean running loss\n",
    "        TrainingLossArray.append(running_loss/i)\n",
    "        \n",
    "        print(\"Training loss: \", running_loss/i)\n",
    "        \n",
    "        # Finding validation loss\n",
    "        validation_loss = 0\n",
    "        for i, data in enumerate(valloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Calculates loss\n",
    "            outputs = model(inputs.cuda())            \n",
    "            loss = criterion(outputs.cuda(), labels.cuda())      \n",
    "            validation_loss += loss.item()\n",
    "        # Appending the mean validation loss\n",
    "        ValidationLossArray.append(validation_loss/i)\n",
    "        print(\"Validation loss: \", validation_loss/i)\n",
    "        \n",
    "        # Initialising params for early stopping\n",
    "        if bestLoss == None:\n",
    "            bestLoss = validation_loss\n",
    "        \n",
    "        # Checks for early stopping\n",
    "        if validation_loss <= bestLoss:\n",
    "            notImproved = 0\n",
    "            bestLoss = validation_loss\n",
    "            bestModel = model\n",
    "            torch.save(bestModel, \"bestModel.pth\")\n",
    "        else:\n",
    "            notImproved +=1\n",
    "        # Converges if the training has not improved for a certain amount of iterations\n",
    "        if notImproved >= early_stopping:\n",
    "            break\n",
    "        \n",
    "    return bestModel, ValidationLossArray, TrainingLossArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eefdc2a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 3.94 GiB total capacity; 2.41 GiB already allocated; 195.81 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2228/1832355033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbestModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationLossArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingLossArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNetTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvNetRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2228/2078842039.py\u001b[0m in \u001b[0;36mConvNetTraining\u001b[0;34m(trainloader, valloader, Conv, lossFunction, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2228/3443909209.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 3.94 GiB total capacity; 2.41 GiB already allocated; 195.81 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "bestModel, ValidationLossArray, TrainingLossArray = ConvNetTraining(trainloader, valloader, ConvNetRGB(), nn.MSELoss(), 0.0001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06dd7829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60006a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b61177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4076b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
